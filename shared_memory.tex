\section{Shared-Memory Model\label{sec:shared_memory}}
A shared-memory concurrency model is any model in which units of computation,
processes or threads, communicate with each other using shared memory.
Communication is done by reading and writing to a shared object. Refer to
\autoref{fig:shared_memory} for a visual representation of the shared-memory model.

% shared memory figure
\begin{figure}[!htp]
    \centering
    \begin{tikzpicture}[node distance=1cm and 1.5cm, >=Stealth]

        % Shared memory bar
        \draw[thick] (0,0) rectangle (11,1);
        \node at (9,0.5) {Shared memory};
      
        % Memory blocks
        \fill[blue!50] (0.3,0.2) rectangle (1.3,0.8);
        \fill[red!70] (2.9,0.2) rectangle (3.9,0.8);
        \fill[blue!50] (5.5,0.2) rectangle (6.5,0.8);
      
        % Processes A and B
        \node[draw, minimum width=1cm, minimum height=1cm] (A) at (1.3,3) {A};
        \node[draw, minimum width=1cm, minimum height=1cm] (B) at (5.7,3) {B};
      
        % Arrows from processes to memory blocks
        \draw[->] (A) -- (0.8,1);
        \draw[->] (A) -- (3.4,1);
        \draw[->] (B) -- (3.4,1);
        \draw[->] (B) -- (6.0,1);
      
      \end{tikzpicture}
      \caption{
        Shared-memory concurrency model. 
        Processes A and B communicate by reading and writing to shared memory blocks.
      }
      \label{fig:shared_memory}
\end{figure}

Note that the definition of \textit{memory} can be loosened depending on the application.
For example,~\cite{mitConcurrency} illustrates the following scenarios
where shared-memory concurrency models are applicable:

\begin{enumerate}
    \item A, B can be two processes communicating using the same process memory.
    \item A, B can be two programs communicating using the same filesystem.
    \item \label{itm:shared_object} A, B can be two running threads of some computer program
        communicating using the same object.
\end{enumerate}

When discussing limitations, advantages, or disadvantages of shared-memory concurrency models,
we will illustrate them using the scenario described in \autoref{itm:shared_object}.

\subsection{The Canonical Example}

The paper in~\cite{jaffe2011impactOfMemoryModels} describes a well-known canonical example.
Assume that we need to update a variable $x$ and increment it by $2$.
Assuming $x$ is already initialized, we can create two threads, $T_1$ and $T_2$, that will
increment $x$ by $1$ each. The code for both threads is shown in \autoref{fig:lost_update}.

Note that if both threads read the value of $x$ at the same time,
the final value of $x$ will be $1$. In fact, there's no guaranteed execution
order between $T_1$, and $T_2$ as the instructions can be interleaved
in any order. The interleaving problem is made more complex
due to the fact that most modern hardware can re-order instructions
\cite{huang2016debuggingConcurrentPrograms}. Furthermore, interleaving
makes it difficult to reproduce bugs, creating \textit{Heisenbugs}~\cite{rookout2021heisenbug,
gray1986whyComputersStop}.

%figure for the canonical example
\begin{figure}[!htp]
    \centering
    \begin{tabular}{|p{0.3\linewidth}|p{0.3\linewidth}|}
        \hline
        \textbf{$T_1$} & \textbf{$T_2$} \\
        \hline
        \begin{Verbatim}[fontsize=\small]
    int loc = x;
    loc = loc + 1;
    x = loc;
        \end{Verbatim}
        &
        \begin{Verbatim}[fontsize=\small]
    int loc = x;
    loc = loc + 1;
    x = loc;
        \end{Verbatim}
        \\
        \hline
    \end{tabular}
    \caption{Canonical example for shared-memory models.}
    \label{fig:lost_update}
\end{figure}

\subsection{Synchronization Mechanisms}
Synchronization mechanisms aim to solve the problems caused by interleaving: race conditions.
As the name implies, synchronization mechanisms synchronize the execution of multiple threads.
These mechanism are used in the critical sections of a program.
The most common synchronization mechanisms include synchronization primitives,
and atomic instructions~\cite{leroy2021sharedMemorySlides}. Note that message passing is a synchronization
mechanism, but it is highly used in the Communicating Sequential Processes (CSP) model.

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Critical Section]
    Critical sections are a series of instructions in a program that cannot
    be interleaved among multiple threads to guarantee the correctness of the program.
\end{tcolorbox}

Synchronization primitives allow
only certain threads to access a shared object at a time.
Locks, mutexes, semaphores, barriers, conditional variables, and monitors
are examples of synchronization primitives.
These mechanism are briefly described in \autoref{tab:mutualExclusionPrimitives}.

\begin{table}[!htp]
    \caption{Comparison of mutual exclusion mechanisms}
    \label{tab:mutualExclusionPrimitives}
    \centering
    \begin{tabularx}{\textwidth}{|l|X|X|}
        \hline
        \textbf{Primitive} & \textbf{Description}\\
        \hline
        Locks & 
        Simples mechanism. Allows only one thread at a time to access a shared object.\\
        \hline
        Mutexes & 
        Similar to a lock, but it can used system wide to synchronize multiple processes.\\
        \hline
        Semaphores & 
        Generalization over a lock: allow $N$ entities to access the resource at a time\\
        \hline
        Barriers &
        Given $N$ threads, all threads block until $P$ threads reach a section.\\
        \hline
        Conditional variables &
        Allows threads to wait for a condition to be true. Enhancement
        over semaphores with wait and signal operations.\\
        \hline
        Monitors &
        High-level abstraction over locks and condition variables. It packages
        the shared object and the synchronization primitives together.\\
        \hline
    \end{tabularx}
\end{table}

The biggest problem with synchronization primitives is that they can
lead to deadlocks when used incorrectly. Note that
the responsibility of avoiding deadlocks falls directly on the developer.
A deadlock occurs when two or more threads are waiting
for each other to release a resource, blocking \textbf{forever}.
When a deadlock occurs, the \textit{progress} property
of the system is violated. All synchronization mechanisms aim
at guaranteeing some of these three properties:
mutual exclusion, progress, and bounded waiting~\cite{csf2025synchronizationPrimitives}.


Given that it is difficult to not only know which synchronization
primitives to used, but also how to use them correctly,
several well-known patterns exist to create
correct concurrent programs: signaling,
turnstiles, rendezvous, multiplexes, and lightswitches~\cite{csf2025synchronizationDesign}.
Please refer to \autoref{tab:basicDesignPattens} for a summary.


% Table for summarizing basic synchronization design patterns
\begin{table}[!htp]
    \caption{Summary of basic synchronization design patterns}
    \label{tab:basicDesignPattens}
    \centering
    \begin{tabularx}{\textwidth}{|l|X|X|}
        \hline
        \textbf{Pattern} & \textbf{Description}\\
        \hline
        Signaling & 
        Use mutexes as a form of signal between two processes. This is
        accomplished by waiting on a shared semaphore that is incremented
        right after the event. Once the semaphore is incremented, the
        the waiting thread can continue execution.\\
        \hline
        Turnstiles & 
        A variant of signaling that can can unblock several threads
        one at a time in a chain-reaction style. Threads wait
        and notify signals one at a time, such that other threads
        can continue execution.\\
        \hline
        Rendezvous & 
        Two threads can signal each other such that they meet
        in a pre-defined point. Two semaphores are needed. Assuming
        semaphores $A, B$ exist. Thread A signals on semaphore $B$ and
        waits on semaphore $A$. Thread B does the same.\\
        \hline
        Multiplexing &
        Used when mutual exclusion is not required, but some
        degree of resource management is needed. It works
        by allowing $N$ threads to access a resource at a time.
        Any other thread must wait.\\
        \hline
        Lightswitches &
        Used when threads can be distinguished by some
        type. For example, threads that perform writes to
        a document and threads that perform I/O of the document
        into disk. Lightswitches allows to synchronize
        threads based on type.\\
        \hline
    \end{tabularx}
\end{table}


\subsection{Memory Consistency Models}
One subtle implication of shared-memory models is that instructions
from all threads must be organized in \textbf{some order}. The reason is straightforward:
they are all accessing the same memory. Given that memory is unique and shared (hardware resource),
multiple threads cannot access the same memory address at the same time. Note that
we make the distinction between instructions and lines of code. The instructions
can be arranged in any order, regardless of how the lines of code are written.

A memory consistency model defines the set of rules that dictate how
instruction can be ordered, or interleaved, in a concurrent
program~\cite{bornholt2016memoryModels,jaffe2011impactOfMemoryModels}.
In concrete, the ordering rules focus on how \textbf{load} and \textbf{store} instructions
can be reordered. A load instruction reads a value from memory,
while a store instruction writes a value to memory.

Sequential Consistency (SC) is the strongest
and most intuitive model ~\cite{lamport1979multiprocessor}.
To guarantee SC, the system must ensure two properties~\cite{jaffe2011impactOfMemoryModels}:
\begin{enumerate}
    \item All threads must see the global order of memory operations.
    \item the operations of a particular property must appear to execute
    in program order.
\end{enumerate}

Although SC is extremely popular due to its high-level of abstraction,
its strictness is detrimental to performance because it limits
optimizations around memory access including caching,
dynamic scheduling, and pipelining. As a result, several relaxed
memory consistency models have been proposed:
Total Store Order (TSO), Partial Store Order (PSO),
and Weak Ordering (WO)~\cite{jaffe2011impactOfMemoryModels}.
Note that most modern hardware architectures implement a relaxed
memory consistency model, with Intel's x86 architecture implementing TSO~\cite{cox2021hardwareMemoryModels}.

Recall that the canonical example in \autoref{fig:lost_update}
demonstrates the interleaving problem. Given that memory
consistency models define how instructions can be rearranged,
they can either exacerbate or mitigate concurrency bugs
caused by undesirable interleaving.

The authors in~\cite{jaffe2011impactOfMemoryModels} show that
as the number of threads increases, the probability
of a concurrency bug manifesting is the same across strict and relaxed
memory consistency models. However, when two threads are used,
weak memory models do increase such probability. The experiment setup
is pretty straightforward: given a critical section for a program
similar to \autoref{fig:lost_update}, and a
memory consistency model, the authors are able to possibly
expand the critical window by reordering instructions.
Then, they are able to measure the probability of a concurrency
by taking into factoring in the size of the critical window
in their probabilistic model.
Note that these findings are not experimental, but rather
mathematically proven.


\subsection{Non-Determinism}

Non-Determinism is the property of a system
where given the same inputs, the system
reaches different states or produces
a different output~\cite{replayDeterminism}.
In shared-memory concurrency models, determinism can be divided in two:

\begin{enumerate}
    \item \textbf{Strong determinism:} All shared memory accesses are deterministically ordered.
    \item \textbf{Weak determinism:} Only the order of lock acquisitions is deterministic.
        This is often referred to as strong determinism for race-free programs.
\end{enumerate}

Shared-memory concurrency programs are susceptible
to exhibit non-deterministic behavior due to
non-deterministic memory access patterns
or lack and incorrect usage of synchronization mechanisms.
These qualities are undesirable when triaging
and fixing bugs in concurrent programs as
creating a minimal reproducible setup which
yields the same failure is invaluable for developers.
Otherwise, developers have to reason about the system
by reading lines of code and attempting
to understand how the systems behaves at runtime. This
approach is not useful when programming a concurrent
program already poses challenges. As a result, several strategies have been proposed
to address the non-determinism nature of
shared-memory concurrent programs.

On one hand, the authors
in~\cite{replayDeterminism} proposed a replay system.
Replay is ability to replicate the steps performed by a system.
Different than having log files, replay allows to examine
stack traces and perform runtime analysis of a previous run.
One interesting aspect of~\cite{replayDeterminism} is that all
events are logged using the same trace mechanism. By doing so,
the runtime overhead is reduced as the instrumentation no longer
needs to distinguish between event types.

On the other hand, authors in~\cite{kendoEfficientDeterministicMultithreading}
introduced Kendo, a C++ software-only system that enforces
deterministic multithreading. One disadvantage
of replay/record systems is that they are costly or infeasible
to maintain post-deployment. Furthermore,
hardware oriented solutions to achieve determinism,
such as the ones proposed in ~\cite{hardwareDeterminism},
are too restrictive because they limit performance
optimizations. Kendo introduces the notion of Deterministic
Logical Time (DLT) where each thread maintains a logical
clock that advances only based in its own actions.
If a thread is using DLT, it must wait no only
the resource to be marked as free but also for its
DLT to become current. Kendo forwards a threads DLT
if possible to avoid wait time.